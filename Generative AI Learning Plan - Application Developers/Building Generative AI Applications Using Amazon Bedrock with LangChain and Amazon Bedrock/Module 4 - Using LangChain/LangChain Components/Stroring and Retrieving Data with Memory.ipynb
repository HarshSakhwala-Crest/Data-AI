{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing and Retrieving Data with Memory\n",
    "- **Memory is essential** for AI assistants to maintain conversational context.\n",
    "- **LLMs do not retain state** between interactions, so **previous messages** must be **included in prompts** for continuity.\n",
    "- **LangChain provides memory components** to store and manage past interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LangChain Memory**\n",
    "- **Purpose**: Stores and summarizes past conversational elements to include in future responses.\n",
    "- **Features**:\n",
    "  - Modular design for **easy integration**.\n",
    "  - Works with other **LangChain components** to build chatbots.\n",
    "  - Supports different **memory types**.\n",
    "\n",
    "### **Supported Memory Types**\n",
    "- **ConversationBufferMemory**:\n",
    "  - The most common memory type.\n",
    "  - Stores **past user-LLM interactions** for reference in future queries.\n",
    "\n",
    "- **ConversationChain**:\n",
    "  - Built on top of **ConversationBufferMemory**.\n",
    "  - Manages entire conversations and maintains **contextual continuity**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain_aws import BedrockLLM\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Initialize Bedrock Client\n",
    "bedrock_client = boto3.client('bedrock-runtime', region_name=\"us-east-1\")\n",
    "\n",
    "# Set up LLM with Amazon Titan\n",
    "titan_llm = BedrockLLM(model_id=\"amazon.titan-tg1-large\", client=bedrock_client)\n",
    "\n",
    "# Initialize Conversation Memory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Create a Conversational Chain with Memory\n",
    "conversation = ConversationChain(\n",
    "    llm=titan_llm, verbose=True, memory=memory\n",
    ")\n",
    "\n",
    "# Initial User Query\n",
    "print(conversation.predict(input=\"Hi! I am in Los Angeles. What are some popular sightseeing places?\"))\n",
    "\n",
    "# Follow-up Question (without mentioning \"Los Angeles\")\n",
    "print(conversation.predict(input=\"What is the closest beach that I can go to?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Key Takeaways**\n",
    "- LLMs do not have built-in memory, requiring external memory components.\n",
    "- LangChain Memory helps store and retrieve past interactions for contextual continuity.\n",
    "- ConversationBufferMemory enables AI assistants to remember previous user-llm interactions.\n",
    "- ConversationChain manages conversations efficiently using stored memory."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
