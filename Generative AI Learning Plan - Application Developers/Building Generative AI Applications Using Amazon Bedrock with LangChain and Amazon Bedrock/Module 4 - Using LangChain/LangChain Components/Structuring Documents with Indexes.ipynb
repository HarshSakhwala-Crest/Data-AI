{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structuring Documents with Indexes\n",
    "Indexes help **structure documents** to optimize **LLM interactions**. In **AWS-based RAG (Retrieval-Augmented Generation) applications**, three key components enable efficient document management:\n",
    "- **Document Loaders** (ingest data from various sources)\n",
    "- **Retrievers** (fetch relevant documents)\n",
    "- **Vector Stores** (store and retrieve embeddings for search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Document Loaders**\n",
    "- **Purpose**: Load documents from different sources to prepare them for embedding and retrieval.\n",
    "- **Supported Sources**: \n",
    "  - **Databases**\n",
    "  - **Online stores**\n",
    "  - **Local file storage**\n",
    "- **Supported File Formats**:\n",
    "  - **HTML**\n",
    "  - **PDF**\n",
    "  - **Code files**\n",
    "  - **Microsoft Office Documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import S3FileLoader\n",
    "\n",
    "# Load a document from S3\n",
    "loader = S3FileLoader(\"mysource_bucket\", \"sample-file.docx\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Retriever**\n",
    "- Purpose: Fetch relevant documents from an indexed dataset.\n",
    "- How It Works:\n",
    "  - User submits a query.\n",
    "  - Retriever searches document index.\n",
    "  - Relevant documents are sent to the LLM for processing.\n",
    "- **AWS Integration:**\n",
    "- Amazon Kendra provides semantic search with pre-built connectors for popular data sources:\n",
    "  - Amazon S3\n",
    "  - SharePoint\n",
    "  - Confluence\n",
    "  - Websites\n",
    "- Supports formats such as HTML, Word, PowerPoint, PDF, Excel, and PureText files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_aws.retrievers import AmazonKendraRetriever\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_aws import ChatBedrock \n",
    "\n",
    "# Define LLM\n",
    "llm = ChatBedrock(\n",
    "    model_kwargs={\"max_tokens_to_sample\": 300, \"temperature\": 1, \"top_k\": 250, \"top_p\": 0.999},\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    ")\n",
    "\n",
    "# Set up the retriever\n",
    "retriever = AmazonKendraRetriever(index_id=kendra_index_id, top_k=5, region_name=region)\n",
    "\n",
    "# Define prompt template\n",
    "prompt_template = \"\"\" \n",
    "Human: This is a friendly conversation between a human and an AI.\n",
    "The AI provides specific details from its context but limits it to 240 tokens.\n",
    "If the AI does not know the answer, it truthfully says it does not know.\n",
    "\n",
    "Assistant: OK, got it, I'll be a truthful AI assistant.\n",
    "\n",
    "Human: Here are a few documents:\n",
    "<documents>\n",
    "{context}\n",
    "</documents>\n",
    "Based on the above documents, answer the following question: {question}\n",
    "Answer 'do not know' if the document does not contain relevant information.\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create a conversational retrieval chain\n",
    "response = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    combine_docs_chain_kwargs={\"prompt\": PROMPT},\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Vector stores**\n",
    "- Purpose: Store and retrieve vector embeddings for efficient semantic search.\n",
    "- How It Works:\n",
    "  - Convert documents into embeddings (numerical representations).\n",
    "  - Store embeddings in a vector database.\n",
    "  - Retrieve relevant embeddings based on user queries.\n",
    "  - Pass the retrieved documents to the LLM for generating accurate responses.\n",
    "- AWS Vector Store Integrations:\n",
    "  - Amazon OpenSearch Serverless\n",
    "  - Amazon Aurora PostgreSQL-Compatible Edition (pgvector extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "from langchain_community.vectorstores import OpenSearchVectorSearch\n",
    "\n",
    "# Get environment variables for index and endpoint\n",
    "index_name = os.environ[\"AOSS_INDEX_NAME\"]\n",
    "endpoint = os.environ[\"AOSS_COLLECTION_ENDPOINT\"]\n",
    "\n",
    "# Initialize Bedrock Embeddings\n",
    "embeddings = BedrockEmbeddings(client=bedrock_client)\n",
    "\n",
    "# Set up the vector store\n",
    "vector_store = OpenSearchVectorSearch(\n",
    "    index_name=index_name,\n",
    "    embedding_function=embeddings,\n",
    "    opensearch_url=endpoint,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    ")\n",
    "\n",
    "# Convert the vector store into a retriever\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Key Takeaways**\n",
    "- Document Loaders: Extract text from various sources (Amazon S3, HTML, PDFs, etc.).\n",
    "- Retrievers: Find relevant documents from an indexed dataset.\n",
    "  - Amazon Kendra enables semantic search and connects to multiple data sources.\n",
    "- Vector Stores: Store and retrieve embeddings for efficient information retrieval.\n",
    "  - Amazon OpenSearch Serverless and pgvector for Amazon Aurora PostgreSQL- Compatible Edition are commonly used."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
