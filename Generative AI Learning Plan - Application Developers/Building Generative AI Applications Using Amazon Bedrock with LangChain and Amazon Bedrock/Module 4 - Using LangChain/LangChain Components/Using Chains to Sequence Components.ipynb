{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Chains to Sequence Components\n",
    "- Chains **connect multiple LangChain components** (LLMs, memory, retrievers, tools).\n",
    "- **Example**: \n",
    "  - First, generate a **blog post** on a topic.\n",
    "  - Then, create a **title** for the blog post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chaining Components**\n",
    "- A **chain** is a **sequence of operations** that run together.\n",
    "- A component in a chain can be:\n",
    "  - A call to an **LLM**.\n",
    "  - An **API request**.\n",
    "  - A **sequence of other chains**.\n",
    "- Example: **LLMChain**\n",
    "  - Takes in an **LLM**, a **prompt template**, and **parameters**.\n",
    "  - Returns the **output of the LLM call**.\n",
    "  - Can **format** and **structure** the output.\n",
    "\n",
    "### **Multi-Step Processing**\n",
    "- A **chain** can use the **output of one LLM call** as the **input to the next call**.\n",
    "- Enables **complex workflows** with multiple LLM interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Processing Large Documents with Chains**\n",
    "- Chains are useful for **handling large data** that exceeds the LLM's context window.\n",
    "- **Example: Document Summarization**\n",
    "  - The document is **split into chunks**.\n",
    "  - The LLM is called **multiple times**.\n",
    "  - A **single summary** is generated from multiple responses.\n",
    "\n",
    "## **Types of Chains in LangChain**\n",
    "1. **LCEL-Based Chains**:\n",
    "   - Built using **LangChain Expression Language (LCEL)**.\n",
    "   - **Primary method** for constructing chains.\n",
    "  \n",
    "2. **Legacy Chains**:\n",
    "   - Constructed by **subclassing** from a legacy `Chain` class.\n",
    "   - Example: `LLMChain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_aws import ChatBedrock as Bedrock\n",
    "\n",
    "# Initialize Chat Model\n",
    "chat = Bedrock(\n",
    "    region_name=\"us-east-1\",\n",
    "    model_kwargs={\"temperature\": 1, \"top_k\": 250, \"top_p\": 0.999, \"anthropic_version\": \"bedrock-2023-05-31\"},\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    ")\n",
    "\n",
    "# Define Prompt Template\n",
    "multi_var_prompt = PromptTemplate(\n",
    "    input_variables=[\"company\"],\n",
    "    template=\"Create a list with the names of the main metrics tracked in the reports of {company}?\",\n",
    ")\n",
    "\n",
    "# Create Chain\n",
    "chain = LLMChain(llm=chat, prompt=multi_var_prompt)\n",
    "\n",
    "# Invoke Chain for Different Inputs\n",
    "answers = chain.invoke(\"Amazon\")\n",
    "print(answers)\n",
    "\n",
    "answers = chain.invoke(\"AWS\")\n",
    "print(answers)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
