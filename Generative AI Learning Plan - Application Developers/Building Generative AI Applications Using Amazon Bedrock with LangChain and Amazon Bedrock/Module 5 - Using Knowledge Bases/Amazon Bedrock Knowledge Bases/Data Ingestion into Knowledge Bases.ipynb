{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion into Knowledge Bases\n",
    "- **Amazon Bedrock Knowledge Bases** integrates **proprietary data** into **generative AI applications**.\n",
    "- The **ingestion process** converts raw data into **vector embeddings** stored in a **vector database**.\n",
    "- This enables **efficient retrieval** for **RAG-based applications**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Ingestion Workflow**\n",
    "1. **Convert raw data into text**.\n",
    "2. **Split text into manageable chunks**.\n",
    "3. **Embed chunks using an embedding model**.\n",
    "4. **Store embeddings in a vector database**.\n",
    "5. **Index data for fast retrieval**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Several factors for RAG Data Retrieval**\n",
    "- Text chunk size \n",
    "- Embedding FM \n",
    "- Vector store (database) type "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Considerations to choose the right chunk size**\n",
    "- **Chunk size** determines the **length of text segments** stored in the vector database.\n",
    "- **Trade-off**:  \n",
    "  - **Smaller chunks**: Faster retrieval but may **lose context**.\n",
    "  - **Larger chunks**: Retain **more context** but **increase computational cost**.\n",
    "\n",
    "#### **Chunking Strategies**\n",
    "| **Strategy** | **Description** |\n",
    "|-------------|----------------|\n",
    "| **Fixed-Length Chunking** | Splits text into **fixed sizes** (tokens, words, or characters). May break context. |\n",
    "| **Sentence-Level Chunking** | Splits text at **sentence boundaries** for better coherence. |\n",
    "| **Semantic Chunking** | Uses NLP techniques to split text **based on meaning and topics**. |\n",
    "| **Hierarchical Chunking** | Organizes documents into **parent-child structures** for complex data (e.g., legal documents). |\n",
    "| **Hybrid Approaches** | Combines multiple chunking strategies for **optimized retrieval**. |\n",
    "\n",
    "ðŸ“Œ *Optimizing chunk size is an **iterative process** that requires testing and fine-tuning.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Considerations to choose an embedding model**\n",
    "- **Embedding models** convert text into **numerical vector representations** for semantic search.\n",
    "- **Factors to consider**:\n",
    "  - **Dimensionality**: Larger embeddings (e.g., **1024 dimensions**) capture **more semantics** but require **higher computation**.\n",
    "  - **Multilingual Support**: Ensure embeddings support **the required languages**.\n",
    "  - **Performance Metrics**: Evaluate retrieval accuracy.\n",
    "  - **Compatibility and Ease of Integration**: Compatibility with **existing systems**.\n",
    "\n",
    "#### **Amazon Bedrock Supported Embeddings Models**\n",
    "| **Model** | **Key Features** |\n",
    "|-----------|-----------------|\n",
    "| **Amazon Titan Text Embedding V2** | Supports **256, 512, and 1024 dimensions**, **99% retrieval accuracy at 512 dimensions**., **97% retrieval accuracy at 256 dimensions**.|\n",
    "| **Cohere Embed v3** | Supports **English and multilingual embeddings** with **1024 dimensions**. |\n",
    "\n",
    "ðŸ“Œ *Choosing the right embedding model is a **trade-off between accuracy, efficiency, and cost**.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Considerations to choose a vector database **\n",
    "- **Vector databases** store **text embeddings** and allow **fast semantic search**.\n",
    "- **Factors to consider**:\n",
    "  - **Data volume and scalability** - vector database should be able to handle large volumes of vector embeddings\n",
    "  - **Query performance** - vector database should provide fast and efficient search capabilities\n",
    "  - **Filter on metadata** - vector database supports metadata filtering\n",
    "  - **Hybrid search** - key benefit of using hybrid search is to get improved quality of retrieved results and expanding the search capabilities.\n",
    "  - **Integration and ecosystem** - integration capabilities of the vector database with your existing technology stack and ecosystem.\n",
    "  - **Deployment options** - deployment options offered by the vector database, such as self-hosted, cloud-hosted, or managed services.\n",
    "  - **Cost and pricing Model** - Analyze the cost and pricing model of the vector database\n",
    "  - **Security and compliance** - Evaluate the security features and compliance certifications offered by the vector database\n",
    "  - **Performance and latency** - Evaluate the performance requirements, including query latency, throughput, and concurrency needs\n",
    "  - **Data availability and durability** - Understand the data availability and durability guarantees offered by the vector database.\n",
    "  \n",
    "#### **Amazon Bedrock Supported Vector Stores**\n",
    "- **OpenSearch Serverless**\n",
    "- **Amazon Aurora PostgreSQL-Compatible Edition**\n",
    "- **Pinecone**\n",
    "- **Redis Cloud**\n",
    "- **MongoDB Atlas**\n",
    "\n",
    "ðŸ“Œ *Choosing the right vector store depends on **retrieval speed, data volume, and integration needs**.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sync documents into Amazon Bedrock Knowledge Bases**\n",
    "- **Amazon Bedrock automates**:\n",
    "  - **Creating, storing, and managing embeddings**.\n",
    "  - **Syncing new documents into the knowledge base**.\n",
    "- **Updating Data**:\n",
    "  - Use **AWS Management Console** or **AWS Lambda function**.\n",
    "  - **Schedule automated ingestion jobs**.\n",
    "  - **Perform incremental data updates**.\n",
    "\n",
    "ðŸ“Œ *After ingestion, the knowledge base is ready for **Retrieve** or **RetrieveAndGenerate API** calls.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Key Takeaways**\n",
    "- **Amazon Bedrock Knowledge Bases** automates **data ingestion and retrieval** for RAG applications.\n",
    "- **Optimizing chunk size, embeddings, and vector databases** improves **retrieval accuracy**.\n",
    "- **Supports multiple embeddings models** (Amazon Titan, Cohere Embed) and **vector databases** (OpenSearch, Pinecone, Redis, etc.).\n",
    "- **AWS Lambda can automate incremental data syncing** to keep **knowledge bases up to date**."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
